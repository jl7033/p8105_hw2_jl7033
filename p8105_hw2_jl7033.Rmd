---
title: "P8105 - Homework 2"
author: "Joe LaRocca"
date: "2024-09-24"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
```

## Problem 1

```{r}

subway_df <- read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names()  %>%
  select(division:entry, vending, ada) %>%
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )

```
##### Part 1: Variables, Data Cleaning, and Rows/Columns

The data contains the name of the subway division, the subway line (the major road the station is located at), the station name, the station's latitude and longitude in degrees (positive means north/east, negative means south/west), each of the routes the station serves (up to 11; larger stations, like those at 34 St/Penn Station and 42 St, have larger numbers of routes served), the type of entrance (stair/door/elevator/escalator/easement), whether the station has a vending machine, and whether the station is ADA compliant.

So far, I've used the `janitor` package to convert each of the column names to all-lowercase, selected the 20 columns that the problem asked for using the `select()` function, and converted the `entry` column into a logical variable using `case_match()` within the `mutate()` function. 

The cleaned-up dataset has a total of `r nrow(subway_df)` rows and `r ncol(subway_df)` columns.

##### Part 2: Answering Questions About the Dataset

```{r}

subway_df %>%
  distinct(line, station_name) %>%
  nrow()

```

Using the `distinct()` function, I found that there are 465 unique station names. I used `distinct()` to create a new $n \times 1$ data frame, where $n$ is the number of distinct subway stations. Then, I simply took the number of rows of the data frame to find the number of distinct stations.

```{r}

subway_df %>%
  select(ada) %>%
  sum()

```

Using the `select` and `sum` functions, I found that there are 468 ADA-compliant stations. Taking advantage of the fact that R counts `TRUE` values as 1 and `FALSE` values as 0 within a logical variable, I simply took the sum of the `ada` column to find the number of stations that are ADA compliant.

```{r}

subway_df %>%
  filter(vending == "NO") %>%
  pull(entry) %>%
  mean()

```

I found that the proportion of station entrances/exits without vending that allow entrance is about 0.377, or 37.7\%. To do so, I used `filter()` to select only the rows without vending, used `pull()` to get the `entry` column as a logical vector, and then took the mean of the `entry` vector to get the proportion of `TRUE` values.

##### Part 3: The A Train

```{r}

subway_tidy_df =
  subway_df %>%
  mutate(route8 = as.character(route8),
         route9 = as.character(route9),
         route10 = as.character(route10),
         route11 = as.character(route11)
         ) %>%
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route",
    names_prefix = "route"
  ) %>%
  filter(!is.na(route))

```

First, I noticed that since some subway routes in New York are represented by numbers (for example, the 1 train), I converted the `route8` through `route11` columns to character in order to maintain consistency (in addition, the `pivot_longer()` function doesn't work if all of the columns selected aren't of the same variable type). Then, I used `pivot_longer()` to separate `route_number`, which gives the index of the route, and `route`, which specifies the precise route of the train. Finally, I used `filter` to select only the rows for which an actual route exists.

```{r}

subway_tidy_df %>%
  distinct(line, station_name, route) %>%
  filter(route == "A") %>%
  nrow()
  
```

I found that there are 60 distinct stations that serve the A train. First, I used `distinct()` to select all unique combinations of line, station name, and route. Then, I filtered the data frame to include only stations which served the A train, and then took the number of rows of the data frame.

```{r}

subway_tidy_df %>%
  distinct(line, station_name, route, ada) %>%
  filter(route == "A" & ada == TRUE) %>%
  nrow()

```

Of the 60 distinct stations that serve the A train, 17 of them are ADA compliant. I used a similar process to the one I used in the previous question, except this time, I included `ada` in the `distinct` function and filtered the data frame by both `route` and `ada`.

## Problem 2

##### Upload and Clean Data

```{r}

mr_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Mr. Trash Wheel",
             skip = 1) %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>%
  select(dumpster:homes_powered) %>%
  mutate(
    sports_balls = as.integer(sports_balls)
    ) %>%
  add_column(wheel = "Mr. Trash Wheel",
             .before = "dumpster") 

prof_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Professor Trash Wheel",
             skip = 1) %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster), !is.na(year)) %>%
  add_column(sports_balls = NA, 
             .before = "homes_powered") %>%
  add_column(wheel = "Professor Trash Wheel",
             .before = "dumpster") %>%
  mutate(
    year = as.character(year)
  )

gwynnda_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Gwynnda Trash Wheel",
             skip = 1) %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>%
  add_column(sports_balls = NA, 
             .before = "homes_powered") %>%
  add_column(wheel = "Gwynnda Trash Wheel",
             .before = "dumpster") %>%
  mutate(
    year = as.character(year)
  )


total_trash_wheel_df = bind_rows(mr_trash_wheel_df, 
                                 prof_trash_wheel_df,
                                 gwynnda_trash_wheel_df)

```

To clean the data, I used `janitor::clean_names()` to convert all variable names to lowercase, used `filter()` to omit the last row of both data frames, which tracks the totals and does not represent single-dumpster data, added the `wheel` variable to distinguish between Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel, added a `sports_balls` column to the Professor Trash Wheel and Gwynnda Trash Wheel data frames and converted the `year` column to a character variable to make the two data frames compatible for binding, and then binded the two data frames together using `bind_rows()`.

The Mr. Trash Wheel data frame has `r nrow(mr_trash_wheel_df)` rows, the Professor Trash Wheel data frame has `r nrow(prof_trash_wheel_df)` rows, and the Gwynnda Trash Wheel has `r nrow(gwynnda_trash_wheel_df)` rows, for a total of `r (nrow(mr_trash_wheel_df) + nrow(prof_trash_wheel_df) + nrow(gwynnda_trash_wheel_df))` unique dumpsters.

#### Find Total Trash Weight Collected by Prof. Trash Wheel

```{r}

prof_trash_wheel_df %>%
  pull(weight_tons) %>%
  sum()

```

Professor Trash Wheel collected a total of 246.74 tons of trash from 2017 to 2023.

#### Find Total Number of Cigarette Butts Collected by Gwynnda in 2022

```{r}

gwynnda_trash_wheel_df %>%
  filter(year == "2022") %>%
  pull(cigarette_butts) %>%
  sum()

```

Gwynnda Trash Wheel collected a total of 205,410 cigarette butts in 2022.

## Problem 3: Great British Bake-Off

#### Importing the Data

```{r}

bakers_df <- read_csv("data/gbb_datasets/bakers.csv") %>%
  janitor::clean_names() 
bakes_df <- read_csv("data/gbb_datasets/bakes.csv") %>%
  janitor::clean_names()
viewers_df <- read_csv("data/gbb_datasets/viewers.csv") %>%
  janitor::clean_names()
results_df <- read_csv("data/gbb_datasets/results.csv", skip = 2) %>%
  janitor::clean_names()
  

```

#### Create a merged dataframe with all relevant GBB data

```{r}

gbb_df = 
  bakers_df %>% 
    separate_wider_delim(baker_name, " ", names = c("baker", "last_name")) %>%
    left_join(results_df, by = c("baker", "series")) %>%
    left_join(bakes_df, by = c("baker", "series", "episode")) %>%
    filter(!is.na(result)) %>%
    select(baker, last_name, series, episode, technical, result, everything()) %>%
    write_csv("data/gbb.csv")

view(gbb_df)

```

Before combining the datasets, I separated the `baker_name` column of the `bakers` data frame into the baker's first and last names for the sake of merging. Then, I used the `left_join()` function to merge the `bakers`, `results`, and `bakes` datasets. Finally, I filtered out the rows for which there was no result, as the baker in question had already been eliminated by that point. For example, Ali Imdad was eliminated in episode 4 of his season, so I filtered out all of the rows for Ali Imdad from episode 5 on. 

One question I have: why is there no data for `bakes` after series 8? I decided to keep the data for series 9 and series 10 because it had important information about result, but the dataset would be more complete if data were available for `bakes` during that time period.

#### Star Baker Table

```{r}

star_baker_df = 
  gbb_df %>%
  filter(result == "STAR BAKER", series >= 5) %>%
  arrange(series, episode) %>%
  view()

head(star_baker_df)

```

In order to make the star baker table more legible, I arranged the entries by series and then by episode. The mean age of the star bakers from series 5 through 10 was `r round(mean(star_baker_df$baker_age), 1)`, slightly lower than the mean age for all bakers, which was `r round(mean(gbb_df$baker_age), 1)`.

#### Organizing the Viewers Dataset

```{r}

viewers_df_tidy = viewers_df %>%
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series", 
    values_to = "viewers",
    names_prefix = "series_"
  ) %>%
  mutate(series = as.numeric(series)) %>%
  filter(!is.na(viewers)) %>%
  select(series, episode, everything()) %>%
  arrange(series, episode)

head(viewers_df_tidy, 10)

```

To tidy up the dataset, I converted `series` into a numeric variable so that I could later arrange the data. I then filtered out episodes with no viewers, since those episodes do not exist. For example, I removed series 1, episodes 7 thorugh 10, because while most seasons of the Great British Bake-Off have 10 episodes, series 1 had only 6. Finally, I reorganized the columns using `select()` so that `series` would come first and be followed by `episode` and used `arrange()` to organize the table by series and then by episode.

#### Finding Average Viewership In Series 1 and Series 5

```{r}

viewers_df_tidy %>%
  filter(series == 1) %>%
  pull(viewers) %>%
  mean()

viewers_df_tidy %>%
  filter(series == 5) %>%
  pull(viewers) %>%
  mean()

```

The average viewership in series 1 was about 2.77 million, while the average viewership in series 5 was about 10.04 million, almost four times the average viewership of series 1.